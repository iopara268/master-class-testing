{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df440527",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01daefcd-e9c2-4644-c01f-5d3982a3c11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "df440527"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfivOFg-t9oN"
      },
      "outputs": [],
      "source": [
        "DATASET_DIR = '/content/drive/MyDrive/Face'"
      ],
      "id": "mfivOFg-t9oN"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_tensor(filepath):\n",
        "    img = Image.open(filepath)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.PILToTensor()\n",
        "    ])\n",
        "    img_tensor = transform(img)\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "img = load_image_tensor(f\"{DATASET_DIR}/train/angry/Training_3908.jpg\").reshape(48, 48)\n",
        "plt.imshow(img,cmap=\"gray\")"
      ],
      "metadata": {
        "id": "S-ZJLG-DreO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "8acb672a-766c-402a-b16f-0ba2d72fb7d3"
      },
      "id": "S-ZJLG-DreO-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x79658f8780a0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGeCAYAAADSRtWEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA040lEQVR4nO3de3DVdXrH8SeQC5fcSCAJ2RDAVbmUgoqAqXuxiDJ2x8KSrrsdZ9Zune5ooyPyh5WZrjvdaQe6nfHWRd22FrudKpad4q7uKlVco7tLuEQYERYWlUsgJOGWC4EkQH79w01qlN/nSfKD/R7g/ZrJjObJ95zf+Z7fOQ8neZ7fkxZFUWQAAPyeDQl9AACAyxMJCAAQBAkIABAECQgAEAQJCAAQBAkIABAECQgAEAQJCAAQBAkIABBEeugD+LTu7m6rr6+3nJwcS0tLC304AIABiqLI2trarLS01IYMEZ9zogvkBz/4QTR+/PgoKysrmj17drRhw4Z+raurq4vMjC+++OKLr4v8q66uTr7fX5BPQC+++KItWbLEnnnmGZszZ449/vjjNn/+fNu1a5cVFRXJtTk5OWZm9sQTT9jw4cPP+TMfffRR7PrW1lZ5+w0NDTLufepqamqKjZ04cUKuPXXqlIxnZ2fHxrx9mzRpkox/7nOfi41lZGTItSNGjJDxrKwsGR85cmRsLEp4KcLTp0/HxuS/vMxs6NChMn7mzBkZV+eK97i8++7u7o6NdXV1ybXt7e0y7q1Xx97R0SHXquP21m/ZskWuzczMlHF1npWWlsq1o0ePlvFhw4bJ+PHjx2NjSX+Tk+Q88+777NmzMq5eQ52dnTL2j//4j73v53EuSAJ69NFH7a/+6q/sW9/6lpmZPfPMM/azn/3M/v3f/90efvhhubZnw4YPHx6bgNTJoDbFzH+z9Z6w9PT4LfPeVLy4um3vuL0kELeX/bntC5mAvDcsz+WYgNR54q3tz32rY0/6hqZ456EXVwnKSyDq9dGf9eofl5dqAurP4/J+5rwXIXR1dVltba3Nmzfv/+9kyBCbN2+erV+//jM/39nZaa2trX2+AACXvvOegI4cOWJnz5614uLiPt8vLi4+56+/li1bZnl5eb1f48aNO9+HBABIQcHLsJcuXWotLS29X3V1daEPCQDwe3De/wY0evRoGzp0qDU2Nvb5fmNjo5WUlHzm57Oysty/IQAALj3nPQFlZmbazJkzbd26dbZw4UIz+/iPouvWrbP77ruv37fT3NwcW1DQ0tISu86rRPN4FUTqj5GHDx+Wa70/+Kk/hI4ZM0auzc3NlXH1B1rvj/XecXtxVfnk3bf3R0z1h2nvj+1JqsHMku2pVyig4knWmvmPSx27949F77ZVQcpVV10l1x49elTGz/UP3B5Jz3HvfUUVw3jFE0kKATxJC6NUIY4qhvEKZXp/rl8/NUBLliyxu+66y66//nqbPXu2Pf7449be3t5bFQcAwAVJQF//+tft8OHD9sgjj1hDQ4Ndc8019tprr32mMAEAcPm6YJfiue+++wb0KzcAwOUleBUcAODyRAICAARBAgIABJFy4xh6fPTRR7Eln6p89uTJk/J2vTLrJCXHeXl5ie5brfcKOLzyWLVnXqmzd000r+RysKWcZn5JsSp/TXqtN0+S8livNFfxSp29PfOeb7VvSR6zmT4PvXPce22rEm+v5N67ULB3rTjFu+8k15/0zgXvvr3XnzoX1HU3+/va4hMQACAIEhAAIAgSEAAgCBIQACAIEhAAIAgSEAAgCBIQACCIlO0DOnnyZGwtubo0uuoLMfN7cbx+GnX76vL8Zv5c+fz8/EHFzPy+EjUSweuX8R6X1zulePedZNSD11/hPS7v2FQPhnffSfbM693wHpdH9fokHQWh9tTrtfFem++//35sbPTo0XJtQUGBjCcZa+A9X957luq38W7b69vy+oSSnksePgEBAIIgAQEAgiABAQCCIAEBAIIgAQEAgiABAQCCIAEBAIJI2T6gvLy82Lp/Vdve2Ngob1fNDOmPJHNcvBkZ2dnZsTFv7keSGS9Ja/29OS3q9ltbW+Va73GpvhOvR8J7Lr09V/0ZXt9Ikrk6Sfu2vPtWj9s7h709U31baj/NzMaMGSPjO3bsGPRt5+bmyniSmT5e72GS89TrIUryfmWmn291HvX3/OYTEAAgCBIQACAIEhAAIAgSEAAgCBIQACAIEhAAIAgSEAAgiJTtA0pPT4+tj1c1997MHa+e36ubV/Xte/fulWvHjh0r42pGjHdcXn/T0aNHY2NezX5eXp6Me7Nt2traYmOq98nM73lRM2K8/gpvvoz3uFSPhNe/5FHrvceVZHaNmd/ro3jzgNSee+e4mgNmZpaTkxMbO3z4sFzr9eqMGDFCxtW5kvQ9ST1f3nOV9P1O9XWpx+ydBz34BAQACIIEBAAIggQEAAiCBAQACIIEBAAIggQEAAgiZcuwoyiKLQFMUuLqXXbdK0k+cuRIbGz48OFyrXc5eVWi6pWgepfgV3FvrXfJd4/aF68M1Ltv7/L/SXjngjr2JGMkvPXecSUtAVfH5pWm97f8djBrd+/eLeNJSqG9+/bOU3WOe6MgvLJ6VSLuHZd3LnivHxVXt93fc5BPQACAIEhAAIAgSEAAgCBIQACAIEhAAIAgSEAAgCBIQACAIFK2D+jMmTPuZePPxeuR8OreGxoaZFwd04QJE+Ra75LuLS0tsTGvV8frc1A9Et6eJOkxMtOXjG9qahr0WjOz/Pz82Jg36uFC9hB5l8H3+pvUeeb1rHh9J97rSh17khEV3vr9+/cP+rjM9Otr3759cq3q7zPTox7MzI4dOzao4zIzW7BggYyrPiHvHPbeD71zScW92+4PPgEBAIIgAQEAgiABAQCCIAEBAIIgAQEAgiABAQCCIAEBAIJI2T6g48ePx/aXqPrzU6dOydvt6OiQ8ZEjR8r4qFGjYmPeXI/Dhw/LuKq593o7vH4aNa/Eq+f37tvrt1GPu6CgQK4tLi6WcdUH4c1Q8npWvH1Rj9ubh+LFVb+Md457j6utrU3Gk8y28fpp1Otvz549cq1336qHz+sx8s4Vb89Vr493Dns9YepcuNDzgNRrQK3tb48dn4AAAEGQgAAAQZCAAABBkIAAAEGQgAAAQZCAAABBpGwZ9oQJE2JHDKjSQO+S7V5ZYlFRkYyrkkivxPvkyZMyrkYqtLe3y7UedQl+rwTVK/Wsr6+X8dGjR8fGVMmvmX9sWVlZsTFvTETS8nNVNuyV5CcZFeFdQj/pmAn1uA8ePCjXeiXeO3fujI15ZdjenqpxKGPHjpVrvefDK23/6U9/GhvzxjF44zHU8+mVcHu814B6v1Tvhd7Yjt7779dPAQBwnpGAAABBkIAAAEGQgAAAQZCAAABBkIAAAEGQgAAAQaRsH9CoUaNie0RU74jXN9La2irjXp9Qc3NzbMzrQfJ6CVR/h9cD4fWsKF7/kupPMvP7GFRPQG1trVzb0tIi46oPqLS0VK71+oSSXKpenSdmZvv27ZNxNRbE653y4vn5+TKuxmccOnRIrt24caOMq14fr1+mrKxMxtV56PWy5ebmyvjnP/95GX/77bdjYxMnTpRrvfPQe19RkvQYedR7kvd+1WPAn4Defvttu/322620tNTS0tLspZde6hOPosgeeeQRGzt2rA0fPtzmzZtnu3fvHujdAAAucQNOQO3t7TZjxgxbsWLFOePf//737cknn7RnnnnGNmzYYCNHjrT58+e7/8oGAFxeBvwruNtuu81uu+22c8aiKLLHH3/c/vZv/9YWLFhgZmY/+tGPrLi42F566SX7xje+kexoAQCXjPNahLBnzx5raGiwefPm9X4vLy/P5syZY+vXrz/nms7OTmttbe3zBQC49J3XBNQzk/3TM9CLi4tj57UvW7bM8vLyer/GjRt3Pg8JAJCigpdhL1261FpaWnq/6urqQh8SAOD34LwmoJKSEjMza2xs7PP9xsbG3tinZWVlWW5ubp8vAMCl77z2AU2cONFKSkps3bp1ds0115jZx303GzZssHvvvXdAt3X69OnYGnY1A8PrJfD+xnTkyBEZV9V8p06dkmu9Xh3Vb+PNgPHmgqh+AK9XwOtf8voY9u/fHxvz+kpUP4yZ3pf33ntPrvV4M5jUuaD6k8z851P1fnh9Pt6vsb3ejwMHDsTGtm/fLtd6/U+TJk2KjU2ePFmu9fph1KyiuH8A9/D6hLzXdmFhYWysoKBArk0y38nrW/R4962o943+9hcNOAGdOHHCPvjgg97/37Nnj23dutUKCgqsvLzcFi9ebH//939vV111lU2cONG+853vWGlpqS1cuHCgdwUAuIQNOAFt3rzZ/viP/7j3/5csWWJmZnfddZc999xz9tBDD1l7e7t9+9vftubmZvvCF75gr732mttNDwC4vAw4Ad10003uR8Lvfe979r3vfS/RgQEALm3Bq+AAAJcnEhAAIAgSEAAgiJQdx3D06NHYUtYkYwu8skOv9FaVPXqlml4JuCrXbGtrk2u9x61KJpOOW0hyufjp06fL+MmTJ2V87969sbGdO3fKtV7JsPe4Fa90PcmeeeeZVxbvPS71GvBaCcrLy2V81qxZsTFvz7z7Vq8fr5y/qKhIxr2yYrXnF7LNwWu/UKNQzPRIETP9fqdu27vf3vvv108BAHCekYAAAEGQgAAAQZCAAABBkIAAAEGQgAAAQZCAAABBpGwfUGNjY2z9u+pb8WruvT4gdYl9M1137/XqeP0bXj+A4vUBqR6JnJwcudbrxfntb38r42pfamtr5Vo1GsDs436xON4l+L3H7fV+qF4H7/L+3tgQdd9e35Y3UsTriZk9e3ZszHuuvdtWz+emTZvkWjXKwUz3GHl9KV4/jdcvo167SccxJB25cKFuO8mIlx58AgIABEECAgAEQQICAARBAgIABEECAgAEQQICAARBAgIABJGyfUBnz56Nrd0/ceJE7LokszV67ldRs1K8WULquM10j4TXX6H6YczMWlpaYmNe/1J9fb2MNzU1ybjaF69X59ixYzI+YcKE2FhxcbFc682X8fqADh48GBvzziPvtlWvT3Z2tlzrzZ365je/KeM33XRTbKympkau/Y//+A8ZLysri42NHj1arvX6ttSex80W6+G9L6jn2kz3vRQWFsq13mwo71xRvD6f/s7tOZck87J68AkIABAECQgAEAQJCAAQBAkIABAECQgAEAQJCAAQBAkIABBEyvYB5eXlxdbuq94Qr+7dq/f31quafa/Pp7m5WcbVvKCks4TU4/JmnXjzSrxZRKqPwZvD4vUJjRs3LjY2fvx4uXbKlCky7s2+OXz4cGzM2zOvlyc/Pz825vWF7Nu3T8aTzKe5/vrr5dqGhgYZV3s6efJkudbrV1N7lpeXJ9d6vTjbtm2T8enTp8u4kuT15a1N2quj+oTUedjf3iU+AQEAgiABAQCCIAEBAIIgAQEAgiABAQCCIAEBAIJI2TLsYcOGxZZhq9LCjIwMebteeaBXoqrKTJNe3l+VVHrl416ZqSpRzc3NlWtHjBgh497jUqMgPvzwQ7nWG3HR0dEh44pXMrx9+3YZVyMT/uAP/kCu9crqR40aFRvzyvnVWjO/tF2VJHttCrfddpuMv/jii7Ex7/m44oorZFy9dvfv3y/X7ty5U8avuuqqQce9c9gbFaFKob3nw4t77RuqRUO9X3nl4b2336+fAgDgPCMBAQCCIAEBAIIgAQEAgiABAQCCIAEBAIIgAQEAgkjZPqD09PTYy5CrPob6+np5u159ujceQPUgeTX3Xh+D6vXx+puKiopkvLS0NDam+gz6w+tpGT16dGyssLBQrl2/fr2Ml5eXx8ZUn46Z30PkXWJf9Ud5e6pGipjpx/XOO+/ItV7PitcHpPqMvJ4Vr1/ti1/8YmzsBz/4gVzrjZlQPSsjR46Ua6+77joZV+ewme6nycnJkWu9URCK14Pn9SZ6o1hUXMW8ES29t9GvnwIA4DwjAQEAgiABAQCCIAEBAIIgAQEAgiABAQCCIAEBAIJI2T6gs2fPxvZSqNp3bxaK17PixdXMkTFjxsi1Xi9CXV1dbMzrMfLmAak+hpMnT8q1nuHDh8u46s2aPHmyXKtmCZnpfhuvZ+XEiRMy7vW0qB4Lr89n4sSJMq7O48OHD8u1X/3qV2X86NGjMu7NiFFUn5yZWXFxcWxswYIFcu3evXtlXL3+vNem91y3trbKuDfLSFGzusz0e453jnv9OF5fpDrH1dr+nkN8AgIABEECAgAEQQICAARBAgIABEECAgAEQQICAASRsmXYg+WVJXrlgd6l6lVZsDfKwbv8vypr9G5bjQbwbttb6/FGD3zuc5+LjXkjEyZNmiTjO3fujI15l9j3LkXvlaer0t2ZM2cOeq2Z2QsvvBAbU2W5ZnqcgpnZrl27ZFyNyPBGC3h7qtoJysrK5Fpvz9S55B2XNxJh0aJFMq5eXxs2bJBrN23aJOO1tbWxMe/9zGuRUGNazPT7YXZ2dmzMK+/uwScgAEAQJCAAQBAkIABAECQgAEAQJCAAQBAkIABAECQgAEAQKdsH1N3dHVtLrnoJvHp/7/LkbW1tMq4uy15eXi7XesaOHRsb27Nnj1zr9eIoXn+S13/h1fyrvhXvvq+++moZb2xsjI29+eabcu20adNk3BvtofZ848aNcu3mzZtlfPz48bGxa6+9Vq71Rlh4IzDUJfi9fhnv+VSvTzVmxczvb1LrVS+aWfLHNWvWrNiY93z9xV/8hYyrc7ympkauXbt2rYyvX79extWYCfW+4D2XPQb0CWjZsmU2a9Ysy8nJsaKiIlu4cOFnmto6OjqsqqrKCgsLLTs72yorK+UGAgAuTwNKQNXV1VZVVWU1NTX2+uuv2+nTp+3WW2+19vb23p958MEH7eWXX7bVq1dbdXW11dfXu13EAIDLz4B+Bffaa6/1+f/nnnvOioqKrLa21r70pS9ZS0uLPfvss/b888/b3Llzzcxs5cqVNmXKFKupqbEbbrjh/B05AOCilqgIoed3zT2/46+trbXTp0/bvHnzen9m8uTJVl5eHvu7xs7OTmttbe3zBQC49A06AXV3d9vixYvtxhtv7P1jbkNDg2VmZn5mxnlxcXHsH7OWLVtmeXl5vV/jxo0b7CEBAC4ig05AVVVV9v7779uqVasSHcDSpUutpaWl96uuri7R7QEALg6DKsO+77777JVXXrG33367zyXUS0pKrKury5qbm/t8CmpsbIy9rHdWVpY7QgEAcOkZUAKKosjuv/9+W7Nmjb311ls2ceLEPvGZM2daRkaGrVu3ziorK83s49kj+/fvt4qKigEdmOoDUr08Q4cOlbfrzVLx5tPs27cvNub1GuTl5cm4mhvi9TcdOHBAxlXNvjdryNtTb8/UsXs9Rl5/Rk+xy7ls27ZNrvV6KLzeDxUfM2aMXHvHHXfI+JVXXhkb+/Wvfy3Xev1o3vOpHtfBgwflWu8fkyru7bea/WSm9zw3N1eu9V4D3mwodZ4eO3ZMrvWej6KiotjYn/3Zn8m1XgXy0aNHZfxXv/pVbOyNN96IjXV1ddl//ud/yts2G2ACqqqqsueff95+8pOfWE5OTu/fdfLy8mz48OGWl5dnd999ty1ZssQKCgosNzfX7r//fquoqKACDgDQx4AS0NNPP21mZjfddFOf769cubK3m/exxx6zIUOGWGVlpXV2dtr8+fPtqaeeOi8HCwC4dAz4V3CeYcOG2YoVK2zFihWDPigAwKWPi5ECAIIgAQEAgiABAQCCIAEBAIJI2XlAmZmZsT0iqub+8OHD8na9undvrk5OTs6g1+7fv1/G1ewN1Rdi5vfLqGvsZWdny7Vef4aaz2RmNnz48NiYd9we9biuuOIKuTauObqH1xui5jepeT5m/gylT485+aR3331XrvWej+3bt8u41+ujqOfazGzkyJGxsdGjR8u1Xh9dfX39oNeOGDFCxr3zVMWT9tGp2/Zem978M2/m1de+9rXY2De+8Y3YWGtra7/6gPgEBAAIggQEAAiCBAQACIIEBAAIggQEAAiCBAQACCJly7BfffXV2PJFVXp74sQJebveyG+v3PLOO++MjXkjE7xLuqtpsJ8effFp3nX6VEmxVwbqlXJ6Zb/qcXslqt6eqvv21nrl515ZsDp2r9zfOxd27NgRG+vs7JRrP/jgAxk/fvy4jCveuAXvXFFlw83NzXKtVzKs9vTUqVNyrXcueCX5Ku6V3Hul1Oq1nXSWmnff6nGp4/LeZ3vwCQgAEAQJCAAQBAkIABAECQgAEAQJCAAQBAkIABAECQgAEETK9gF99NFHsT0eqg/C64dR4xTMzO655x4Z//M///PY2BtvvCHXepd8b2lpiY15/Un5+fkyrkZFeGMk4sZi9PD6bZKs9fqEFK//IiMjQ8a93hF1rnm9Ol4vjuqjmDBhglxbUFAg42VlZTKu+nG8vhFvz86cORMb81673vOp1nvneJLzzEz3y3g9REl6eZKMiTDzX39qvIbqH+zvmBU+AQEAgiABAQCCIAEBAIIgAQEAgiABAQCCIAEBAIIgAQEAgkjZPiDVT6D6IO644w55u7fffruMe31Cas7L+PHj5do9e/bIuJo/481C8WapqFkpXi9A0rjqz/B6cbxZQ6o3xOu/8HpDvF4etd67b29ulXrceXl5cq03k8frt1H9HV6fj+ob8W7b65Pz4kVFRbGx9vZ2udY7z7zHpfqIvHO8vz0z55K0d8o7NvW4VU+Y1y/Wg09AAIAgSEAAgCBIQACAIEhAAIAgSEAAgCBIQACAIFK2DPuhhx6KvUx5ZWVl7DpVimlm1tDQIONNTU0yrkpvp0yZItd6pbk7d+6MjY0cOVKu9crHDx48OOjbVqWzZn4paCjecXnlr14Ja5IRF14ZtlrvXb7fGy2gxn54911SUiLXeiXg6ra90l3vHM/Ozo6NeY/Zo27bTJ8rXouEt2dqhIVqrzDzz4XCwkIZV+XraqSId3734BMQACAIEhAAIAgSEAAgCBIQACAIEhAAIAgSEAAgCBIQACCIlO0Duueee2Lr/lW/gOp3MfMvy+5ddl3FvXp+r4ciMzMzNrZjxw651qMuN3/s2DG5tri4WMa9nhdvXIPiXSZf8fp4vD4h1X9hph+3d9veeajOca+3w9szb5yDGkPh9Z14IyzUa8DrN1N9J2b6+fJem15vlbdejWnxXh/eeabOBfWeYeY/rra2NhlXz6e6b+8x9+ATEAAgCBIQACAIEhAAIAgSEAAgCBIQACAIEhAAIAgSEAAgiJTtAzpx4kRsP4OaNeH1QHhzPby6eVX77s0c8XqMpk2bFhvzZqXU1dXJuOo78fpKkvYxqD3zZiR51PPt9eIkmfdjpo/du2+v/0L1xHi37fXieH1fp06dio3NmjVLrh0zZoyMl5eXD+p+zcx+/OMfy7jivfZGjBgh4965ol4D3lrv+VLvWd5tJ5lvZqb3Rc3T8vrcevAJCAAQBAkIABAECQgAEAQJCAAQBAkIABAECQgAEAQJCAAQRMr2AaWlpcX2eKjZHN58DK9PyOtpGex8DDO/50X1fvzRH/2RXKvmkZiZvfDCC7ExVc9v5vcJJdmzJPN+zJLNGvJ6ILweCxVP2vuhZva0trbKtV7PWGlpqYxfffXVsbErr7xSrvX29Le//W1s7KWXXpJrvV63efPmxcaSzn7yXiNqvXffXm+i6hnz5p9NmDBBxr33LHXs3vtCf/AJCAAQBAkIABAECQgAEAQJCAAQBAkIABAECQgAEETKlmFHURRbAqjKsL3yV68kMiMjQ8ZV2a9Xquldgt8rBVVU2a6Z2TXXXBMbe/311+Va7xL7niSPyyvTTlLGnXRcg7pvrzzcK2FV63/zm9/ItV5p7ciRI2X8V7/6VWzsZz/7mVzr7WlDQ0NsbO/evXLtlClTZFw9Lq903RvX4O2ZWu/tiWq/MNPl5xs3bpRrDx8+LONlZWUyrsYxqMd8QcYxPP300zZ9+nTLzc213Nxcq6iosFdffbU33tHRYVVVVVZYWGjZ2dlWWVlpjY2NA7kLAMBlYkAJqKyszJYvX261tbW2efNmmzt3ri1YsMC2b99uZmYPPvigvfzyy7Z69Wqrrq62+vp6W7Ro0QU5cADAxW1Av4K7/fbb+/z/P/zDP9jTTz9tNTU1VlZWZs8++6w9//zzNnfuXDMzW7lypU2ZMsVqamrshhtuOH9HDQC46A26COHs2bO2atUqa29vt4qKCqutrbXTp0/3uRzG5MmTrby83NavXx97O52dndba2trnCwBw6RtwAtq2bZtlZ2dbVlaW3XPPPbZmzRqbOnWqNTQ0WGZmpuXn5/f5+eLiYvmHx2XLllleXl7v17hx4wb8IAAAF58BJ6BJkybZ1q1bbcOGDXbvvffaXXfdZTt27Bj0ASxdutRaWlp6v7wLDgIALg0DLsPOzMzsvSLuzJkzbdOmTfbEE0/Y17/+devq6rLm5uY+n4IaGxutpKQk9vaysrIsKytr4EcOALioJe4D6u7uts7OTps5c6ZlZGTYunXrrLKy0szMdu3aZfv377eKiooB3+6QIUNieyFUn4PXc+JdLt6ryVf37ZWce4lW1dx7j+vEiRMyri6xX1NTI9d6owOSSNIjZJZ8nEOo+/b6To4fPx4b835L4PWjffDBBzKuzkMVMzMrKCiQ8alTp8bGvH4Z7/Wjxhp4fSmqt7A/1OvP68vyqPEZM2bMkGu3bdsm495vr9T4jVmzZsm1/TGgXV+6dKnddtttVl5ebm1tbfb888/bW2+9ZWvXrrW8vDy7++67bcmSJVZQUGC5ubl2//33W0VFBRVwAIDPGFACampqsm9+85t26NAhy8vLs+nTp9vatWvtlltuMTOzxx57zIYMGWKVlZXW2dlp8+fPt6eeeuqCHDgA4OI2oAT07LPPyviwYcNsxYoVtmLFikQHBQC49HExUgBAECQgAEAQJCAAQBAkIABAECk7DygtLS22z0L1X5w+fVrerjfX49OXEvq0AwcOxMa82RtJepC83g6vf0nFp02bJtd6M0cmTZok483NzTKudHR0yLg6F7y+Ea8HydtzdS55t+2dh+pc8XpWvB4jb1/UbCnvtr0+INWU/uGHH8q1OTk5Mq5451FxcbGMnzx5UsaPHDkSG8vNzZVrvfcs9dpduHChXHvHHXfI+BtvvCHjR48ejY2pc8F7r+vBJyAAQBAkIABAECQgAEAQJCAAQBAkIABAECQgAEAQKVuGPXToUBs6dOg5Y2o8QGFhobxd79Lou3fvlnF1Kfuk5a/q2OJGU/TwLmWvylCnT58u13qXdPdKVNXj9spj486BHqrcuaWlRa71zgXvvk+dOhUb884F7/lS5cyzZ8+Waw8ePCjj3kgFVeY9atQoudYrw04y9iBJGbZXFuxNYi4rK5NxdS54Zdbe6+fYsWOxMW8/J0+eLOPenqrXgHrM3uu6B5+AAABBkIAAAEGQgAAAQZCAAABBkIAAAEGQgAAAQZCAAABBpGwfUFZWVuxlyJNcBv/QoUMyri4/bqbr7r3+Cq/vRPX6eD0pXlxd0n38+PFyrddLUFNTI+NTpkyJjXnH3dbWJuPd3d2xMTWqwcw/V9RtmyXr2/LOBdUndOWVV8q13miOJD1IHu81oJ5P7zz04ur58npxvH4Yb0/V7XvnmfcaGDNmTGzMO8/27t0r42q8jJnuKVM9l6pH6JP4BAQACIIEBAAIggQEAAiCBAQACIIEBAAIggQEAAiCBAQACCJl+4BOnDgR28ehegm8univH8CLq5p+r2/Eq/dX/RleT4vXf+HNxlG+9KUvyfjmzZtlvKmpKTaWm5sr12ZnZ8u4mqXizSTx9jTJDKYk55GZ7jvxeqO8mVhqvox3+16PkHds+/fvj42VlpbKtd7zoeaEeX08GRkZMu7Nd1K3397eLtd6PWH5+fmxMe+59l4DY8eOlXHVJ9TQ0DDo++3BJyAAQBAkIABAECQgAEAQJCAAQBAkIABAECQgAEAQKVuGfezYMevq6jpnTJUzZ2Vlydv1ygO9y4ir+/bKrM+ePSvjqozbKwP1jluVenrH5ZVqfuELX5Dx1atXx8auueYaudYrlfZKWBWvbN4rzVVlv0nGY5jp0t3Ro0fLtR6vdFftiyq99daamU2dOjU2pkadmPml7eo89vbbO8+89WpEjNcC4Y2CUOeSakMw80dveG0On//852NjW7ZsiY2p18Yn8QkIABAECQgAEAQJCAAQBAkIABAECQgAEAQJCAAQBAkIABBEyvYBpaenx/YFqF4fry7++PHjMu5d8l1dRr+5uVmu9S437/UwKV4/jHpcXq+A12O0YMECGa+pqYmNqV4CM7Nx48bJuOKNqPB6Vrx9Ueu9frMk55nXD+P1tHj9NqrnzOtB8m5b9VZ5Ywu8fjW1L97rw+vb8s4V1Qfk3faJEydkXB27d1ze4/b6CydOnBgbU8+H9z7cg09AAIAgSEAAgCBIQACAIEhAAIAgSEAAgCBIQACAIEhAAIAgUrYPqLu7O7bGXdWYe/0XSXs/4mYU9ee2vRkZar132968EtV34vUKeP00Tz75pIxPmDAhNrZ371651uutUo9bPVdmZvn5+TLuUf023nnoPV8qfvToUbnWm/HiUa8v7/Xh9X+oXh1vT3Jzc2Vc9dsUFRXJtd457vUg5eXlxca8vixvxpJ63xgzZoxc670GvPcVdeyqR8jr6erBJyAAQBAkIABAECQgAEAQJCAAQBAkIABAECQgAEAQJCAAQBAp2wfU1dUVW8OuavK9Xhuvnt+j6ua9OS1qxouZ7h0ZNWqUXOs9rpycnNjY7t275dqHHnpIxn/+85/LeGVlZWzsa1/7mlz7L//yLzJ+1VVXxca8/guvZ8Xrj0oyl8qLq14erxfHm3nlzadRvSPefat5P2b+HCTFu2/1fHvPpffa9V5faq6O14N0+PBhGW9qaoqNec9lYWGhjHuziNR8J/V+xjwgAEBKIwEBAIIgAQEAgiABAQCCIAEBAIIgAQEAgkjZMuzu7u7Y0sekpdTe/Q42nmTcgpkuI/XKREeOHCnjO3bsiI15ZdZr166VcXUpejOz6urq2Nj06dPl2oqKChl/++23Y2NTp06Va5OM3jDTl/D3SsBbWlpkXJXNq5iZf9zeeap457DXaqDOFW/cgip1NtOPq6SkRK71ni9vT1UpdUFBgVw7efJkGd+8eXNs7MCBA3JtkvYMM/18qj35vZRhL1++3NLS0mzx4sW93+vo6LCqqiorLCy07Oxsq6ystMbGxiR3AwC4BA06AW3atMl++MMffuZfsA8++KC9/PLLtnr1aquurrb6+npbtGhR4gMFAFxaBpWATpw4YXfeeaf967/+a58O/ZaWFnv22Wft0Ucftblz59rMmTNt5cqV9utf/9pqamrO20EDAC5+g0pAVVVV9pWvfMXmzZvX5/u1tbV2+vTpPt+fPHmylZeX2/r16895W52dndba2trnCwBw6RtwEcKqVavs3XfftU2bNn0m1tDQYJmZmZafn9/n+8XFxbFzz5ctW2Z/93d/N9DDAABc5Ab0Caiurs4eeOAB+6//+i8bNmzYeTmApUuXWktLS+9XXV3debldAEBqG1ACqq2ttaamJrvuuussPT3d0tPTrbq62p588klLT0+34uJi6+rqsubm5j7rGhsbY8sgs7KyLDc3t88XAODSN6Bfwd188822bdu2Pt/71re+ZZMnT7a/+Zu/sXHjxllGRoatW7eu9xL8u3btsv3797v9HJ/W2dkZeylwVdvu9Xaoy4ub+f026va9Hggvri6tri7Pb2b2zjvvyPj//M//xMauuOIKufaGG26Q8S1btsi48uijj8r4vffeK+MzZsyIjaneJzOz8vJyGff+MXT06NHYmNdX4p1nqr9j4sSJcq3XE+b1aKhz3BtroHqjzPS+eGu9nhY1tsB7zF5/U9yfEHqosQbeb4u8USuzZs2Kjf3yl7+Ua3fu3CnjXn+UOrZTp04NKvZJA0pAOTk5Nm3atD7fGzlypBUWFvZ+/+6777YlS5ZYQUGB5ebm2v33328VFRXumxgA4PJy3q+E8Nhjj9mQIUOssrLSOjs7bf78+fbUU0+d77sBAFzkEiegt956q8//Dxs2zFasWGErVqxIetMAgEsYFyMFAARBAgIABEECAgAEQQICAASRsvOAOjo6YvtiVA+F6qUx8/uAvF4dda06r/Zd9QqY6X6BDz/8UK5duXKljI8fPz425h33tddeK+Ofbjz+tH379sXGvJ6V//7v/5bx+fPnx8a8q2qo4zIzKyoqkvGsrKzYWJLZT2ZmbW1tsbGDBw/KtWPHjpVxb36TOnavN8rrb1KvL2+2TUZGhoyrPfMqcb0+ultuuUXGi4uLY2PezB2vt0qtv/766+VarxfO23P13lBYWBgbGzKkf59t+AQEAAiCBAQACIIEBAAIggQEAAiCBAQACIIEBAAIIqXLsONK+bxSaqWlpUXGvfJZVZbY1dUl1yYpUV27dq1c6x23Knfu6OiQa73LxX/xi1+UcXVsXkmxdxl9tS+fHhn/aV6Jt7en6jz0xmd4l+hXoweOHDky6LVmunzWTJdpqxEUZh+PUVFUG8Px48fl2quvvlrG1etLlcybmRUUFMi493yqcmfVAmHml7arNofhw4fLtdddd52Me8/Xp8fvfNLevXtjY957Sg8+AQEAgiABAQCCIAEBAIIgAQEAgiABAQCCIAEBAIIgAQEAgkjZPqAjR47E1rh7NfmKql038y/5HkVRbMyrffdGRRw6dCg29v7778u13iXf1eNSj8lba2aWn58v4zfeeGNs7M0335RrGxsbZXzPnj2xMW8cg9e/9Nprr8l4WVlZbEyNBjDze17UnnuXulfnkZk/FkT1lnj9NElemzNmzJBxb4zErl27YmPens2ZM0fGR48eLeP79++PjXljP7weJNUz5j0fI0aMkHHvfUPtizqHvf69HnwCAgAEQQICAARBAgIABEECAgAEQQICAARBAgIABEECAgAEkbJ9QG1tbbHzPVQfgzdzx+sN8eZrlJSUxMa82RpeP42aKeL1GHm9BJmZmbExrw/Im4vjzQtS62+99Va59uc//7mMq3lC77zzjly7aNEiGZ8yZYqM19fXx8a83ig1n8lMz/RJS0uTa9X8GDN/Ps1NN90UG/P6l7zXj4o3NTXJtevWrZNxdS784R/+oVw7bdo0Gfdmfam5O1u3bpVrvT6hoqKi2JjX3+Txni/12la9Ud57Sg8+AQEAgiABAQCCIAEBAIIgAQEAgiABAQCCIAEBAIIgAQEAgkjZPqC0tLTYfgfVE9Pa2ipv1+tpUf0XPccVx5vN4c2IUX0Q6en6qfLq+ZP0lZw6dUrGvZ4WNSPGez7mzp0r4z/96U9jY17f1RtvvCHjXu+ImlXkzUPxnk/1nHjzfNT8GDOzP/3TP5VxtW9jxoyRa71+mVdeeSU25vXLeNTrL8ksLu+2zfSeXXvttXKtN+tLnWde/5/3uL2eMfVeq97PvPeMHnwCAgAEQQICAARBAgIABEECAgAEQQICAARBAgIABJGyZdhnzpyxM2fOnDOmSlS9kQjeZcK9cQ6qvNAr1dy9e7eMq/JaVcps5pf1qmPzSs+9uEeViHtlu+pS9GZmf/InfxIb27Jli1y7d+9eGd+4caOMjxs3LjZ24MABudajzlOvdP2WW26Rca90Vz0n3vP1ox/9SMa3b98eG/POca+sXr0v7Ny5U67dtm2bjE+aNEnG1bHl5eXJtTfeeKOMv/fee7GxDz/8UK71xoKMGDFCxtW5plpevPExPfgEBAAIggQEAAiCBAQACIIEBAAIggQEAAiCBAQACCLlyrB7yk9VGZ8qDfTKsL0yUq/EVR2Xd1Vpr8RbHbt3217Zoyof9/bMK8Pub8nlYO7be75U3DturyQ/yZXTvbVePEkZtrdn3pW61Xnq3XZc60SPJI8rSTzpOeztmSrD9lokvPMwyWvXe1ze+4p3bN79euvTosHewwVy4MAB2V8BALg41NXVWVlZWWw85RJQd3e31dfXW05OjqWlpVlra6uNGzfO6urqLDc3N/ThXRTYs4FjzwaOPRu4y2XPoiiytrY2Ky0ttSFD4v/Sk3K/ghsyZMg5M2Zubu4l/YRdCOzZwLFnA8eeDdzlsGfeFSDMKEIAAARCAgIABJHyCSgrK8u++93vuhf6xP9jzwaOPRs49mzg2LO+Uq4IAQBweUj5T0AAgEsTCQgAEAQJCAAQBAkIABAECQgAEETKJ6AVK1bYhAkTbNiwYTZnzhzbuHFj6ENKGW+//bbdfvvtVlpaamlpafbSSy/1iUdRZI888oiNHTvWhg8fbvPmzbPdu3eHOdgUsGzZMps1a5bl5ORYUVGRLVy40Hbt2tXnZzo6OqyqqsoKCwstOzvbKisrrbGxMdARp4ann37apk+f3tu9X1FRYa+++mpvnD3Tli9fbmlpabZ48eLe77FnH0vpBPTiiy/akiVL7Lvf/a69++67NmPGDJs/f741NTWFPrSU0N7ebjNmzLAVK1acM/7973/fnnzySXvmmWdsw4YNNnLkSJs/f36iq1dfzKqrq62qqspqamrs9ddft9OnT9utt95q7e3tvT/z4IMP2ssvv2yrV6+26upqq6+vt0WLFgU86vDKysps+fLlVltba5s3b7a5c+faggULbPv27WbGnimbNm2yH/7whzZ9+vQ+32fPfidKYbNnz46qqqp6///s2bNRaWlptGzZsoBHlZrMLFqzZk3v/3d3d0clJSXRP/3TP/V+r7m5OcrKyopeeOGFAEeYepqamiIzi6qrq6Mo+nh/MjIyotWrV/f+zG9+85vIzKL169eHOsyUNGrUqOjf/u3f2DOhra0tuuqqq6LXX389+vKXvxw98MADURRxnn1Syn4C6urqstraWps3b17v94YMGWLz5s2z9evXBzyyi8OePXusoaGhz/7l5eXZnDlz2L/faWlpMTOzgoICMzOrra2106dP99mzyZMnW3l5OXv2O2fPnrVVq1ZZe3u7VVRUsGdCVVWVfeUrX+mzN2acZ5+UclfD7nHkyBE7e/asFRcX9/l+cXGx7dy5M9BRXTwaGhrMzM65fz2xy1l3d7ctXrzYbrzxRps2bZqZfbxnmZmZlp+f3+dn2TOzbdu2WUVFhXV0dFh2dratWbPGpk6dalu3bmXPzmHVqlX27rvv2qZNmz4T4zz7fymbgIALqaqqyt5//3375S9/GfpQLgqTJk2yrVu3WktLi/34xz+2u+66y6qrq0MfVkqqq6uzBx54wF5//XUbNmxY6MNJaSn7K7jRo0fb0KFDP1MZ0tjYaCUlJYGO6uLRs0fs32fdd9999sorr9gvfvGLPrOnSkpKrKury5qbm/v8PHtmlpmZaVdeeaXNnDnTli1bZjNmzLAnnniCPTuH2tpaa2pqsuuuu87S09MtPT3dqqur7cknn7T09HQrLi5mz34nZRNQZmamzZw509atW9f7ve7ublu3bp1VVFQEPLKLw8SJE62kpKTP/rW2ttqGDRsu2/2Losjuu+8+W7Nmjb355ps2ceLEPvGZM2daRkZGnz3btWuX7d+//7Ldszjd3d3W2dnJnp3DzTffbNu2bbOtW7f2fl1//fV255139v43e/Y7oasglFWrVkVZWVnRc889F+3YsSP69re/HeXn50cNDQ2hDy0ltLW1RVu2bIm2bNkSmVn06KOPRlu2bIn27dsXRVEULV++PMrPz49+8pOfRO+99160YMGCaOLEidGpU6cCH3kY9957b5SXlxe99dZb0aFDh3q/Tp482fsz99xzT1ReXh69+eab0ebNm6OKioqooqIi4FGH9/DDD0fV1dXRnj17ovfeey96+OGHo7S0tOh///d/oyhiz/rjk1VwUcSe9UjpBBRFUfTP//zPUXl5eZSZmRnNnj07qqmpCX1IKeMXv/hFZGaf+brrrruiKPq4FPs73/lOVFxcHGVlZUU333xztGvXrrAHHdC59srMopUrV/b+zKlTp6K//uu/jkaNGhWNGDEi+upXvxodOnQo3EGngL/8y7+Mxo8fH2VmZkZjxoyJbr755t7kE0XsWX98OgGxZx9jHhAAIIiU/RsQAODSRgICAARBAgIABEECAgAEQQICAARBAgIABEECAgAEQQICAARBAgIABEECAgAEQQICAATxf7n5B3jhzgVoAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPoeHrsMiSIO"
      },
      "outputs": [],
      "source": [
        "Image_list = []\n",
        "Label_list = []\n",
        "label_dic = {\"angry\":0, \"disgust\":1, \"fear\":2, \"happy\":3, \"neutral\":4, \"sad\":5, \"surprise\":6,}\n",
        "for folder in os.listdir(DATASET_DIR):\n",
        "    for label in os.listdir(f\"{DATASET_DIR}/\"+folder):\n",
        "        for image in os.listdir(f\"{DATASET_DIR}/\"+folder+\"/\"+label):\n",
        "            Image_list.append(f\"{DATASET_DIR}/\"+folder+\"/\"+label+\"/\"+image)\n",
        "            Label_list.append(label_dic[label])\n",
        "\n",
        "Image_df = pd.DataFrame(Image_list,columns=[\"Image Path\"])\n",
        "Label_df = pd.DataFrame(Label_list,columns=[\"Label\"])\n",
        "Dataset_df = pd.concat([Image_df,Label_df],axis=1)"
      ],
      "id": "NPoeHrsMiSIO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmlRQDwiPAjG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FERDataset(Dataset):\n",
        "    def __init__(self, dataset_df):\n",
        "        super().__init__()\n",
        "        self.dataframe = dataset_df[:35880]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataframe.iloc[i]"
      ],
      "id": "CmlRQDwiPAjG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09bGDHrJPRDi",
        "outputId": "81c3856e-aeb6-470f-ea79-d120b99d622f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: File not found at Face/train/angry/Training_3908.jpg\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing as mp\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    image_batch_tensor = torch.FloatTensor(len(batch), 48, 48)\n",
        "    image_tensors = []\n",
        "    labels = []\n",
        "    for item in batch:\n",
        "        image_tensor = load_image_tensor(item[0])\n",
        "        image_tensors.append(image_tensor)\n",
        "        labels.append(item[1])\n",
        "\n",
        "\n",
        "    torch.cat(image_tensors, out=image_batch_tensor)\n",
        "    label_batch_tensor = torch.LongTensor(labels)\n",
        "    return (image_batch_tensor, label_batch_tensor)\n",
        "\n",
        "\n",
        "def load_data(dataset_df, batch_sz=100, train_val_test_split=[0.3, 0.2, 0.5]):\n",
        "\n",
        "    assert sum(train_val_test_split) == 1, \"Train, val and test fractions should sum to 1!\"\n",
        "    dataset = FERDataset(dataset_df)\n",
        "\n",
        "    tr_va_te = []\n",
        "    for frac in train_val_test_split:\n",
        "        actual_count = frac * len(dataset)\n",
        "        actual_count = round(actual_count)\n",
        "        tr_va_te.append(actual_count)\n",
        "\n",
        "    # split dataset into train, val and test\n",
        "    train_split, val_split, test_split = random_split(dataset, tr_va_te)\n",
        "\n",
        "\n",
        "    n_cpus = mp.cpu_count() # returns number of CPU cores on this machine\n",
        "    train_dl = DataLoader(train_split,\n",
        "                          batch_size=batch_sz,\n",
        "                          shuffle=True,\n",
        "                          collate_fn=custom_collate_fn,\n",
        "                          num_workers=n_cpus)\n",
        "    val_dl = DataLoader(val_split,\n",
        "                        batch_size=batch_sz,\n",
        "                        shuffle=True,\n",
        "                        collate_fn=custom_collate_fn,\n",
        "                        num_workers=n_cpus)\n",
        "    test_dl = DataLoader(test_split,\n",
        "                         batch_size=batch_sz,\n",
        "                         shuffle=False,\n",
        "                         collate_fn=custom_collate_fn,\n",
        "                         num_workers=n_cpus)\n",
        "    return train_dl, val_dl, test_dl\n"
      ],
      "id": "09bGDHrJPRDi"
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl, val_dl, test_dl = load_data(Dataset_df, batch_sz=4)"
      ],
      "metadata": {
        "id": "VLor29c4r23_"
      },
      "id": "VLor29c4r23_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdXqLGA4Pjok"
      },
      "outputs": [],
      "source": [
        "def image_grid(batch, ncols=4):\n",
        "    height, width = batch[0].shape\n",
        "    nrows = len(batch)//ncols # calculate the number of rows based on the number of columns needed by the user\n",
        "\n",
        "    img_grid = (batch.reshape(nrows, ncols, height, width)\n",
        "              .swapaxes(1,2)\n",
        "              .reshape(height*nrows, width*ncols))\n",
        "\n",
        "    return img_grid\n",
        "\n",
        "\n",
        "def show_batch(batch, title=\"Image batch\", cols=4):\n",
        "    N = len(batch)\n",
        "    if N > cols:\n",
        "        assert N % cols == 0, \"Number of cols must be a multiple of N\"\n",
        "\n",
        "    result = image_grid(batch, cols)\n",
        "    fig = plt.figure(figsize=(4., 4.))\n",
        "    plt.suptitle(f\"{title} [{int(N/cols)}x{cols}]\")\n",
        "    plt.imshow(result, cmap='gray')"
      ],
      "id": "VdXqLGA4Pjok"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGWNk40vi81i"
      },
      "outputs": [],
      "source": [
        "train_images, _ = next(iter(train_dl))\n",
        "test_images, _ = next(iter(test_dl))\n",
        "\n",
        "show_batch(train_images, title=\"Train images\", cols=4)\n",
        "show_batch(test_images, title=\"Test images\", cols=4)"
      ],
      "id": "IGWNk40vi81i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFC6l9umQI_y",
        "outputId": "d32f5050-4ed9-40a0-c113-b7d45901e839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Face/train/angry/Training_3908.jpg\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n"
      ],
      "id": "EFC6l9umQI_y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om1j90ZmP5jM",
        "outputId": "ed32155e-f493-470f-a4af-646aa440392f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "def train_model_gpu_lr(model, epochs, train_dl, optimiser, lr_scheduler, device):\n",
        "    msg = \"\"\n",
        "    for epoch in range(epochs):\n",
        "        total_steps = len(train_dl)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        model.train()\n",
        "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
        "            batch_sz = len(image_batch)\n",
        "\n",
        "            image_batch = image_batch.to(device)\n",
        "            label_batch = label_batch.to(device)\n",
        "            output = model(image_batch)\n",
        "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
        "\n",
        "            optimiser.zero_grad()\n",
        "            losses.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            correct += int(torch.eq(preds, label_batch).sum())\n",
        "            total += batch_sz\n",
        "            minibatch_accuracy = 100 * correct / total\n",
        "\n",
        "\n",
        "            if (batch_num + 1) % 5 == 0:\n",
        "              print(\"\\r \" * len(msg), end='')\n",
        "              msg = f'\\rTrain epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
        "              print (msg, end='' if epoch < epochs else \"\\n\",flush=True)"
      ],
      "id": "Om1j90ZmP5jM"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "\n",
        "def get_simple_conv_net():\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(16 * 14 * 14, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 10)\n",
        "    )\n",
        "\n",
        "model = get_simple_conv_net()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "summary(model, input_size=(1, 28, 28))\n"
      ],
      "metadata": {
        "id": "m13UQRdFsLjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408d0445-c63e-435a-d149-bef20bc2581b"
      },
      "id": "m13UQRdFsLjv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             416\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "           Flatten-4                 [-1, 3136]               0\n",
            "            Linear-5                  [-1, 128]         401,536\n",
            "              ReLU-6                  [-1, 128]               0\n",
            "            Linear-7                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 403,242\n",
            "Trainable params: 403,242\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.24\n",
            "Params size (MB): 1.54\n",
            "Estimated Total Size (MB): 1.78\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_gpu_lr_conv(model, epochs, train_dl, optimiser, lr_scheduler):\n",
        "    msg = \"\"\n",
        "    for epoch in range(epochs):\n",
        "        total_steps = len(train_dl)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
        "            batch_sz = len(image_batch)\n",
        "            label_batch = label_batch.to(DEVICE)\n",
        "            image_batch = image_batch.to(DEVICE).reshape(batch_sz, 1, 28, 28)\n",
        "            output = model(image_batch)\n",
        "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
        "\n",
        "            optimiser.zero_grad()\n",
        "            losses.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            correct += int(torch.eq(preds, label_batch).sum())\n",
        "            total += batch_sz\n",
        "            minibatch_accuracy = 100 * correct / total\n",
        "\n",
        "\n",
        "            if (batch_num + 1) % 5 == 0:\n",
        "                print(\" \" * len(msg), end='\\r')\n",
        "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
        "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
        "\n",
        "        lr_scheduler.step()"
      ],
      "metadata": {
        "id": "U4Cn3lBTsSfv"
      },
      "id": "U4Cn3lBTsSfv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "4xK9Vvv_hSP9",
        "outputId": "724dd61e-f4b3-4526-c1c8-a410def26e5c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-860aca4123bf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_simple_conv_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr_sch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExponentialLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
          ]
        }
      ],
      "source": [
        "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
        "network = get_simple_conv_net()\n",
        "optim = SGD(network.parameters(), lr=learning_rate)\n",
        "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
        "network = network.to(DEVICE)\n",
        "train_model_gpu_lr_conv(network, epochs, train_dl, optim, lr_sch)\n"
      ],
      "id": "4xK9Vvv_hSP9"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATA_PATH = \"DATASET_DIR\"\n",
        "\n",
        "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
        "network = get_simple_conv_net()\n",
        "optim = SGD(network.parameters(), lr=learning_rate)\n",
        "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
        "network = network.to(DEVICE)\n",
        "train_model_gpu_lr_conv(network, epochs, train_dl, optim, lr_sch)\n"
      ],
      "metadata": {
        "id": "9aQPUWWNsZl2"
      },
      "id": "9aQPUWWNsZl2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample):\n",
        "        super().__init__()\n",
        "        if downsample:\n",
        "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "            self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        shortcut = self.shortcut(inp)\n",
        "        inp = nn.ReLU()(self.bn1(self.conv1(inp)))\n",
        "        inp = nn.ReLU()(self.bn2(self.conv2(inp)))\n",
        "        inp = inp + shortcut  # The magic bit that cannot be done with nn.Sequential!\n",
        "        return nn.ReLU()(inp)\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels, resblock, outputs):\n",
        "        super().__init__()\n",
        "        self.layer0_conv = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.layer0_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer0_bn   = nn.BatchNorm2d(64)\n",
        "        self.layer0_relu = nn.ReLU()\n",
        "\n",
        "        self.layer1_res1 = resblock(64, 64, downsample=False)\n",
        "        self.layer1_res2 = resblock(64, 64, downsample=False)\n",
        "\n",
        "        self.layer2_res1 = resblock(64, 128, downsample=True)\n",
        "        self.layer2_res2 = resblock(128, 128, downsample=False)\n",
        "\n",
        "        self.layer3_res1 = resblock(128, 256, downsample=True)\n",
        "        self.layer3_res2 = resblock(256, 256, downsample=False)\n",
        "\n",
        "        self.layer4_res1 = resblock(256, 512, downsample=True)\n",
        "        self.layer4_res2 = resblock(512, 512, downsample=False)\n",
        "\n",
        "        self.gap         = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flat        = nn.Flatten()\n",
        "        self.fc          = nn.Linear(512, outputs)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        inp = self.layer0_conv(inp)\n",
        "        inp = self.layer0_pool(inp)\n",
        "        inp = self.layer0_bn(inp)\n",
        "        inp = self.layer0_relu(inp)\n",
        "\n",
        "        inp = self.layer1_res1(inp)\n",
        "        inp = self.layer1_res2(inp)\n",
        "\n",
        "        inp = self.layer2_res1(inp)\n",
        "        inp = self.layer2_res2(inp)\n",
        "\n",
        "        inp = self.layer3_res1(inp)\n",
        "        inp = self.layer3_res2(inp)\n",
        "\n",
        "        inp = self.layer4_res1(inp)\n",
        "        inp = self.layer4_res2(inp)\n",
        "\n",
        "        inp = self.gap(inp)\n",
        "        inp = self.flat(inp)\n",
        "        inp = self.fc(inp)\n",
        "\n",
        "        return inp\n",
        "\n",
        "\n",
        "# convenience function\n",
        "def get_resnet():\n",
        "    return ResNet(1, ResBlock, outputs=10)\n",
        "\n",
        "\n",
        "# Print model summary\n",
        "summary(get_resnet(), input_size=(1, 28, 28), device=\"cpu\")"
      ],
      "metadata": {
        "id": "XblMIpKAsV4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b372d627-a994-4f11-aa72-467a0255a414"
      },
      "id": "XblMIpKAsV4n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 14, 14]           3,200\n",
            "         MaxPool2d-2             [-1, 64, 7, 7]               0\n",
            "       BatchNorm2d-3             [-1, 64, 7, 7]             128\n",
            "              ReLU-4             [-1, 64, 7, 7]               0\n",
            "            Conv2d-5             [-1, 64, 7, 7]          36,928\n",
            "       BatchNorm2d-6             [-1, 64, 7, 7]             128\n",
            "            Conv2d-7             [-1, 64, 7, 7]          36,928\n",
            "       BatchNorm2d-8             [-1, 64, 7, 7]             128\n",
            "          ResBlock-9             [-1, 64, 7, 7]               0\n",
            "           Conv2d-10             [-1, 64, 7, 7]          36,928\n",
            "      BatchNorm2d-11             [-1, 64, 7, 7]             128\n",
            "           Conv2d-12             [-1, 64, 7, 7]          36,928\n",
            "      BatchNorm2d-13             [-1, 64, 7, 7]             128\n",
            "         ResBlock-14             [-1, 64, 7, 7]               0\n",
            "           Conv2d-15            [-1, 128, 4, 4]           8,320\n",
            "      BatchNorm2d-16            [-1, 128, 4, 4]             256\n",
            "           Conv2d-17            [-1, 128, 4, 4]          73,856\n",
            "      BatchNorm2d-18            [-1, 128, 4, 4]             256\n",
            "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "         ResBlock-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
            "           Conv2d-24            [-1, 128, 4, 4]         147,584\n",
            "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
            "         ResBlock-26            [-1, 128, 4, 4]               0\n",
            "           Conv2d-27            [-1, 256, 2, 2]          33,024\n",
            "      BatchNorm2d-28            [-1, 256, 2, 2]             512\n",
            "           Conv2d-29            [-1, 256, 2, 2]         295,168\n",
            "      BatchNorm2d-30            [-1, 256, 2, 2]             512\n",
            "           Conv2d-31            [-1, 256, 2, 2]         590,080\n",
            "      BatchNorm2d-32            [-1, 256, 2, 2]             512\n",
            "         ResBlock-33            [-1, 256, 2, 2]               0\n",
            "           Conv2d-34            [-1, 256, 2, 2]         590,080\n",
            "      BatchNorm2d-35            [-1, 256, 2, 2]             512\n",
            "           Conv2d-36            [-1, 256, 2, 2]         590,080\n",
            "      BatchNorm2d-37            [-1, 256, 2, 2]             512\n",
            "         ResBlock-38            [-1, 256, 2, 2]               0\n",
            "           Conv2d-39            [-1, 512, 1, 1]         131,584\n",
            "      BatchNorm2d-40            [-1, 512, 1, 1]           1,024\n",
            "           Conv2d-41            [-1, 512, 1, 1]       1,180,160\n",
            "      BatchNorm2d-42            [-1, 512, 1, 1]           1,024\n",
            "           Conv2d-43            [-1, 512, 1, 1]       2,359,808\n",
            "      BatchNorm2d-44            [-1, 512, 1, 1]           1,024\n",
            "         ResBlock-45            [-1, 512, 1, 1]               0\n",
            "           Conv2d-46            [-1, 512, 1, 1]       2,359,808\n",
            "      BatchNorm2d-47            [-1, 512, 1, 1]           1,024\n",
            "           Conv2d-48            [-1, 512, 1, 1]       2,359,808\n",
            "      BatchNorm2d-49            [-1, 512, 1, 1]           1,024\n",
            "         ResBlock-50            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-51            [-1, 512, 1, 1]               0\n",
            "          Flatten-52                  [-1, 512]               0\n",
            "           Linear-53                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,180,170\n",
            "Trainable params: 11,180,170\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.74\n",
            "Params size (MB): 42.65\n",
            "Estimated Total Size (MB): 43.39\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "DATASET_DIR = 'data/images'\n",
        "\n",
        "def load_image_tensor(filepath):\n",
        "    img = Image.open(filepath)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.PILToTensor()\n",
        "    ])\n",
        "    img_tensor = transform(img)\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "img = load_image_tensor(f\"{DATASET_DIR}/train/angry/3905.jpg\").reshape(48, 48)\n",
        "plt.imshow(img,cmap=\"gray\")\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.text(50, 50, \"Angry\", color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n",
        "plt.show()\n",
        "\n",
        "Image_list = []\n",
        "Label_list = []\n",
        "label_dic = {\"angry\":0, \"disgust\":1, \"fear\":2, \"happy\":3, \"neutral\":4, \"sad\":5, \"surprise\":6,}\n",
        "for folder in os.listdir(DATASET_DIR):\n",
        "    for label in os.listdir(f\"{DATASET_DIR}/\"+folder):\n",
        "        for image in os.listdir(f\"{DATASET_DIR}/\"+folder+\"/\"+label):\n",
        "            Image_list.append(f\"{DATASET_DIR}/\"+folder+\"/\"+label+\"/\"+image)\n",
        "            Label_list.append(label_dic[label])\n",
        "\n",
        "Image_df = pd.DataFrame(Image_list,columns=[\"Image Path\"])\n",
        "Label_df = pd.DataFrame(Label_list,columns=[\"Label\"])\n",
        "Dataset_df = pd.concat([Image_df,Label_df],axis=1)\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FERDataset(Dataset):\n",
        "    def __init__(self, dataset_df):\n",
        "        super().__init__()\n",
        "        # self.dataframe = dataset_df[:35880]\n",
        "        self.dataframe = dataset_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataframe.iloc[i]\n",
        "\n",
        "import multiprocessing as mp\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    image_batch_tensor = torch.FloatTensor(len(batch), 48, 48)\n",
        "    image_tensors = []\n",
        "    labels = []\n",
        "    for item in batch:\n",
        "        image_tensor = load_image_tensor(item.iloc[0])\n",
        "        image_tensors.append(image_tensor)\n",
        "        labels.append(item.iloc[1])\n",
        "\n",
        "    torch.cat(image_tensors, out=image_batch_tensor)\n",
        "    label_batch_tensor = torch.LongTensor(labels)\n",
        "    return (image_batch_tensor, label_batch_tensor)\n",
        "\n",
        "\n",
        "def load_data(dataset_df, batch_sz=100, train_val_test_split=[0.3, 0.2, 0.5]):\n",
        "\n",
        "    assert sum(train_val_test_split) == 1, \"Train, val and test fractions should sum to 1!\"\n",
        "    dataset = FERDataset(dataset_df)\n",
        "\n",
        "    tr_va_te = []\n",
        "    for frac in train_val_test_split:\n",
        "        actual_count = frac * len(dataset)\n",
        "        actual_count = round(actual_count)\n",
        "        tr_va_te.append(actual_count)\n",
        "\n",
        "    # split dataset into train, val and test\n",
        "    train_split, val_split, test_split = random_split(dataset, tr_va_te)\n",
        "\n",
        "\n",
        "    n_cpus = mp.cpu_count()\n",
        "    train_dl = DataLoader(train_split,\n",
        "                          batch_size=batch_sz,\n",
        "                          shuffle=True,\n",
        "                          collate_fn=custom_collate_fn,\n",
        "                          num_workers=n_cpus)\n",
        "    val_dl = DataLoader(val_split,\n",
        "                        batch_size=batch_sz,\n",
        "                        shuffle=True,\n",
        "                        collate_fn=custom_collate_fn,\n",
        "                        num_workers=n_cpus)\n",
        "    test_dl = DataLoader(test_split,\n",
        "                         batch_size=batch_sz,\n",
        "                         shuffle=False,\n",
        "                         collate_fn=custom_collate_fn,\n",
        "                         num_workers=n_cpus)\n",
        "    return train_dl, val_dl, test_dl\n",
        "\n",
        "def image_grid(batch, ncols=4):\n",
        "    height, width = batch[0].shape\n",
        "    nrows = len(batch)//ncols\n",
        "    img_grid = (batch.reshape(nrows, ncols, height, width)\n",
        "              .swapaxes(1,2)\n",
        "              .reshape(height*nrows, width*ncols))\n",
        "\n",
        "    return img_grid\n",
        "\n",
        "\n",
        "def show_batch(batch, title=\"Image batch\", cols=4):\n",
        "    N = len(batch)\n",
        "    if N > cols:\n",
        "        assert N % cols == 0, \"Number of cols must be a multiple of N\"\n",
        "\n",
        "    result = image_grid(batch, cols)\n",
        "    fig = plt.figure(figsize=(4., 4.))\n",
        "    plt.suptitle(f\"{title} [{int(N/cols)}x{cols}]\")\n",
        "    plt.imshow(result, cmap='gray')\n",
        "\n",
        "train_dl, val_dl, test_dl = load_data(Dataset_df, batch_sz=16)\n",
        "train_images, _ = next(iter(train_dl))\n",
        "test_images, _ = next(iter(test_dl))\n",
        "\n",
        "show_batch(train_images, title=\"Train images\", cols=4)\n",
        "show_batch(test_images, title=\"Test images\", cols=4)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "\n",
        "def get_simple_conv_net():\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(16 * 24 * 24, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 10)\n",
        "    )\n",
        "\n",
        "model = get_simple_conv_net()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "summary(model, input_size=(1, 48, 48))\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "batch_sz = 16\n",
        "learning_rate = 0.001\n",
        "gamma = 0.8\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "epochs = 50\n",
        "\n",
        "\n",
        "train_dl, val_dl, test_dl = load_data(Dataset_df, batch_sz=4)\n",
        "\n",
        "network = get_simple_conv_net()\n",
        "optim = SGD(network.parameters(), lr=learning_rate)\n",
        "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
        "network = network.to(DEVICE)\n",
        "train_model_gpu_lr_conv(network, epochs, train_dl, optim, lr_sch)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=1, tolerance=0):\n",
        "       self.patience = patience\n",
        "       self.tolerance = tolerance\n",
        "       self.epoch_counter = 0\n",
        "       self.max_validation_acc = np.NINF\n",
        "\n",
        "    def should_stop(self, validation_acc):\n",
        "        if validation_acc > self.max_validation_acc:\n",
        "            self.max_validation_acc = validation_acc\n",
        "            self.epoch_counter = 0\n",
        "        elif validation_acc < (self.max_validation_acc - self.tolerance):\n",
        "            self.epoch_counter += 1\n",
        "            if self.epoch_counter >= self.patience:\n",
        "                return True\n",
        "        return False\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "def train_model_final(model,\n",
        "                      epochs,\n",
        "                      dataloaders,\n",
        "                      optimiser,\n",
        "                      lr_scheduler,\n",
        "                      writer,\n",
        "                      early_stopper,\n",
        "                      checkpoint_frequency):\n",
        "    msg = \"\"\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "        train_dl = dataloaders['train']\n",
        "\n",
        "        total_steps_train = len(train_dl)\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        loss_train = 0\n",
        "\n",
        "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
        "            batch_sz = len(image_batch)\n",
        "            label_batch = label_batch.to(DEVICE)\n",
        "            image_batch = image_batch.to(DEVICE).reshape(batch_sz, 1, 48, 48)\n",
        "            output = model(image_batch)\n",
        "            loss_train = nn.CrossEntropyLoss()(output, label_batch)\n",
        "\n",
        "            optimiser.zero_grad()\n",
        "            loss_train.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            preds_train = torch.argmax(output, dim=1)\n",
        "            correct_train += int(torch.eq(preds_train, label_batch).sum())\n",
        "            total_train += batch_sz\n",
        "            minibatch_accuracy_train = 100 * correct_train / total_train\n",
        "\n",
        "\n",
        "            if (batch_num + 1) % 5 == 0:\n",
        "                print(\" \" * len(msg), end='\\r')\n",
        "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_train}], Loss: {loss_train.item():.5f}, Acc: {minibatch_accuracy_train:.5f}'\n",
        "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
        "\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "USING ALL THSES CODES AND FUNCTIONS, WRITE A SIMPLE UNIT TEST CODE THAT WILL RUN"
      ],
      "metadata": {
        "id": "AFKzXwhjbbyz"
      },
      "id": "AFKzXwhjbbyz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"from torch import (\n",
        "    load_data,\n",
        "    get_simple_conv_net,\n",
        "    train_model_gpu_lr_conv,\n",
        "    EarlyStopper,\n",
        "    train_model_final,\n",
        ")"
      ],
      "metadata": {
        "id": "9PHaMUNUA5st"
      },
      "id": "9PHaMUNUA5st",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We do not need to define a new train function as the only changes have been to the internal\n",
        "# structure of the model, not any of its inputs or outputs, so we can keep using\n",
        "# the old one - very handy!\n",
        "\n",
        "# loading data\n",
        "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
        "dataloaders = {\n",
        "    'train': train_dl,\n",
        "    'val': val_dl,\n",
        "    'test': test_dl\n",
        "}\n",
        "\n",
        "\n",
        "# Instantiate the res net\n",
        "network = get_resnet()\n",
        "# Send it to the GPU device\n",
        "network = network.to(DEVICE)\n",
        "optim = SGD(network.parameters(), lr=learning_rate)\n",
        "lr_sch = ExponentialLR(optim, gamma)\n",
        "# Call the lateset training function from last week\n",
        "train_model_gpu_lr_conv_valid(network, epochs, dataloaders, optim, lr_sch)"
      ],
      "metadata": {
        "id": "35HZHMdbsd9P"
      },
      "id": "35HZHMdbsd9P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=1, tolerance=0):\n",
        "       self.patience = patience # How many epochs in a row the model is allowed to underperform\n",
        "       self.tolerance = tolerance # How much leeway the model has (i.e. how close it can get to underperforming before it is counted as such)\n",
        "       self.epoch_counter = 0 # Keeping track of how many epochs in a row were failed\n",
        "       self.max_validation_acc = np.NINF # Keeping track of best metric so far\n",
        "\n",
        "    def should_stop(self, validation_acc):\n",
        "        if validation_acc > self.max_validation_acc:\n",
        "            self.max_validation_acc = validation_acc\n",
        "            self.epoch_counter = 0\n",
        "        elif validation_acc < (self.max_validation_acc - self.tolerance):\n",
        "            self.epoch_counter += 1\n",
        "            if self.epoch_counter >= self.patience:\n",
        "                return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "JQq-E2mmsiDn"
      },
      "id": "JQq-E2mmsiDn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "nzVpfm3hsnsf"
      },
      "id": "nzVpfm3hsnsf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# Saves a model to file, and names it after the current epoch\n",
        "def save_checkpoint(model, epoch, save_dir):\n",
        "    filename = f\"checkpoint_{epoch}.pth\"\n",
        "    save_path = f\"{save_dir}/{filename}\"\n",
        "    torch.save(model.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "kd6dvSdUspin"
      },
      "id": "kd6dvSdUspin",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_final(model,\n",
        "                      epochs,\n",
        "                      dataloaders,\n",
        "                      optimiser,\n",
        "                      lr_scheduler,\n",
        "                      writer,\n",
        "                      early_stopper,\n",
        "                      checkpoint_frequency):\n",
        "    msg = \"\"\n",
        "    for epoch in range(epochs):\n",
        "        #######################TRAINING STEP###################################\n",
        "        model.train()  # set model to training mode\n",
        "        train_dl = dataloaders['train'] # select train dataloader\n",
        "\n",
        "        total_steps_train = len(train_dl)\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        loss_train = 0\n",
        "\n",
        "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
        "            batch_sz = len(image_batch)\n",
        "            label_batch = label_batch.to(DEVICE)\n",
        "            image_batch = image_batch.to(DEVICE).reshape(batch_sz, 1, 28, 28)\n",
        "            output = model(image_batch)\n",
        "            loss_train = nn.CrossEntropyLoss()(output, label_batch)\n",
        "\n",
        "            optimiser.zero_grad()\n",
        "            loss_train.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            preds_train = torch.argmax(output, dim=1)\n",
        "            correct_train += int(torch.eq(preds_train, label_batch).sum())\n",
        "            total_train += batch_sz\n",
        "            minibatch_accuracy_train = 100 * correct_train / total_train\n",
        "\n",
        "            #### Fancy printing stuff, you can ignore this! ######\n",
        "            if (batch_num + 1) % 5 == 0:\n",
        "                print(\" \" * len(msg), end='\\r')\n",
        "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_train}], Loss: {loss_train.item():.5f}, Acc: {minibatch_accuracy_train:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
        "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
        "            #### Fancy printing stuff, you can ignore this! ######\n",
        "        lr_scheduler.step()\n",
        "        ########################################################################\n",
        "        print(\"\") # Create newline between progress bars\n",
        "        #######################VALIDATION STEP##################################\n",
        "        model.eval()  # set model to evaluation mode. This is very important, we do not want to update model weights in eval mode\n",
        "        val_dl = dataloaders['val'] # select val dataloader\n",
        "\n",
        "        total_steps_val = len(val_dl)\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        loss_val = 0\n",
        "\n",
        "        for batch_num, (image_batch, label_batch) in enumerate(val_dl):\n",
        "            batch_sz = len(image_batch)\n",
        "            label_batch = label_batch.to(DEVICE)\n",
        "            image_batch = image_batch.to(DEVICE).reshape(batch_sz, 1, 28, 28)\n",
        "\n",
        "            with torch.no_grad(): # no_grad disables gradient calculations, which are not needed when evaluating the model. This speeds up the calculations\n",
        "                output = model(image_batch)\n",
        "                loss_val = nn.CrossEntropyLoss()(output, label_batch)\n",
        "\n",
        "                preds_val = torch.argmax(output, dim=1)\n",
        "                correct_val += int(torch.eq(preds_val, label_batch).sum())\n",
        "                total_val += batch_sz\n",
        "                minibatch_accuracy_val = 100 * correct_val / total_val\n",
        "\n",
        "                #### Fancy printing stuff, you can ignore this! ######\n",
        "                if (batch_num + 1) % 5 == 0:\n",
        "                    print(\" \" * len(msg), end='\\r')\n",
        "                    msg = f'Eval epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_val}], Loss: {loss_val.item():.5f}, Acc: {minibatch_accuracy_val:.5f}'\n",
        "                    if early_stopper.epoch_counter > 0:\n",
        "                        msg += f\", Epochs without improvement: {early_stopper.epoch_counter}\"\n",
        "                    print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
        "                #### Fancy printing stuff, you can ignore this! ######\n",
        "        ########################################################################\n",
        "        print(\"\")  # Create newline between progress bars\n",
        "\n",
        "        # Log loss and accuracy metrics using the writer so we can see them in Tensorboard\n",
        "        epoch_train_acc = 100 * correct_train / total_train\n",
        "        epoch_val_acc = 100 * correct_val / total_val\n",
        "\n",
        "        writer.add_scalar('Loss/train', loss_train, epoch)\n",
        "        writer.add_scalar('Loss/val', loss_val, epoch)\n",
        "        writer.add_scalar('Acc/train', epoch_train_acc, epoch)\n",
        "        writer.add_scalar('Acc/val', epoch_val_acc, epoch)\n",
        "\n",
        "        # Check whether we need to save the model to a checkpoint file\n",
        "        if epoch % checkpoint_frequency == 0:\n",
        "            save_checkpoint(model, epoch, \"./saved_models\")\n",
        "        # Check whether we should stop the training based on the validation accuracy\n",
        "        if early_stopper.should_stop(epoch_val_acc):\n",
        "            print(f\"\\nValidation accuracy has not improved in {early_stopper.epoch_counter} epochs, stopping.\")\n",
        "            # if stopping, we also want to save the model's state\n",
        "            save_checkpoint(model, epoch, \"./saved_models\")\n",
        "            return"
      ],
      "metadata": {
        "id": "8gM1w_wEsrYm"
      },
      "id": "8gM1w_wEsrYm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting it all together now, we can launch the final training run!\n",
        "epochs =   15            # increase epochs to show off early stopper\n",
        "batch_sz = 128           # increase batch size for faster processing\n",
        "checkpoint_frequency = 3 # save model to a file every 3 epochs\n",
        "\n",
        "\n",
        "# data loading with new batch size\n",
        "\n",
        "dataloaders = {\n",
        "    'train': train_dl,\n",
        "    'val': val_dl,\n",
        "    'test': test_dl\n",
        "}\n",
        "\n",
        "network = get_resnet()\n",
        "optim = SGD(network.parameters(), lr=learning_rate)  # Stochastic gradient descent optimiser\n",
        "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
        "network = network.to(DEVICE)\n",
        "\n",
        "writer = SummaryWriter()\n",
        "stopper = EarlyStopper(patience=3, tolerance=0) # stop training if model accuracy is not better than the max so far 3 times in a row\n",
        "train_model_final(network, epochs, dataloaders, optim, lr_sch, writer, stopper, checkpoint_frequency)"
      ],
      "metadata": {
        "id": "B0c8b1ZTs1Em",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "f34553d3-d3b8-4f39-c025-0720e292f72f"
      },
      "id": "B0c8b1ZTs1Em",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dl' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-090a13e66b13>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m dataloaders = {\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dl' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da71aed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "593c0dfe-31ca-447b-a7d3-d93005412e14"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c2d072a7b450>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlast_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_net_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./saved_models/checkpoint_{last_epoch}.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m dataloaders = {\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "last_epoch = 10\n",
        "loaded_net_state_dict = torch.load(f\"./saved_models/checkpoint_{last_epoch}.pth\")\n",
        "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
        "dataloaders = {\n",
        "    'train': train_dl,\n",
        "    'val': val_dl,\n",
        "    'test': test_dl\n",
        "}"
      ],
      "id": "da71aed7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20eb4b30"
      },
      "outputs": [],
      "source": [
        "def test_model(model, dataloaders):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    test_dl = dataloaders['test']\n",
        "    total_steps = len(test_dl)\n",
        "    msg = \"\"\n",
        "    for batch_num, (image_batch, label_batch) in enumerate(test_dl):\n",
        "        batch_sz = len(image_batch)\n",
        "        label_batch = label_batch.to(DEVICE)\n",
        "        image_batch = image_batch.to(DEVICE).reshape(batch_sz, 1, 28, 28)\n",
        "        out = model(image_batch)\n",
        "        preds = torch.argmax(out, dim=1)\n",
        "        correct += int(torch.eq(preds, label_batch).sum())\n",
        "        total += label_batch.shape[0]\n",
        "        if (batch_num + 1) % 5 == 0:\n",
        "            print(\" \" * len(msg), end='\\r')\n",
        "            msg = f'Testing batch[{batch_num + 1}/{total_steps}]'\n",
        "            print (msg, end='\\r' if batch_num < total_steps else \"\\n\", flush=True)\n",
        "    print(f\"\\nFinal test accuracy for {total} examples: {100 * correct/total:.5f}\")"
      ],
      "id": "20eb4b30"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02ce501e"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl"
      ],
      "id": "02ce501e"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}